{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zOHwUr7pwCJB"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import os\n",
    "from PIL import Image\n",
    "from IPython.display import display \n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.models as models\n",
    "import torchsummary\n",
    "import numpy as np\n",
    "from collections import namedtuple\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchsummary import summary\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import copy\n",
    "import time\n",
    "import glob\n",
    "import re\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1213,
     "status": "ok",
     "timestamp": 1575469395962,
     "user": {
      "displayName": "김성연",
      "photoUrl": "",
      "userId": "11703282769554310998"
     },
     "user_tz": -540
    },
    "id": "KCTCqdNmwCJI",
    "outputId": "2c982ad6-6054-4391-a90b-246064bccb05"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 2381,
     "status": "ok",
     "timestamp": 1575467079710,
     "user": {
      "displayName": "김성연",
      "photoUrl": "",
      "userId": "11703282769554310998"
     },
     "user_tz": -540
    },
    "id": "gEOvxVk9wwdR",
    "outputId": "6007ed03-5717-46cb-bd2d-f1bc08b63544"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/gdrive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "huK8QKI7wCJM"
   },
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "random_seed = 10\n",
    "num_epochs = 128\n",
    "initial_lr = 1e-3\n",
    "checkpoint_dir = \"/content/gdrive/My Drive/Colab_Notebooks/best-artworks-of-all-time/data/\" \n",
    "\n",
    "log_interval = 20\n",
    "checkpoint_interval = 200\n",
    "\n",
    "#running_option = \"test\" \n",
    "running_option = \"training\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J9-D3H9ywCJP"
   },
   "outputs": [],
   "source": [
    "class ConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride):\n",
    "        super(ConvLayer, self).__init__()\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.reflection_pad(x)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "\n",
    "    \n",
    "    \n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, channels):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.conv1 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in1 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.conv2 = ConvLayer(channels, channels, kernel_size=3, stride=1)\n",
    "        self.in2 = nn.InstanceNorm2d(channels, affine=True)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        out = self.relu(self.in1(self.conv1(x)))\n",
    "        out = self.in2(self.conv2(out))\n",
    "        out = out + residual\n",
    "        return out    \n",
    "    \n",
    "\n",
    "\n",
    "class UpsampleConvLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, upsample=None):\n",
    "        super(UpsampleConvLayer, self).__init__()\n",
    "        self.upsample = upsample\n",
    "        reflection_padding = kernel_size // 2\n",
    "        self.reflection_pad = nn.ReflectionPad2d(reflection_padding)\n",
    "        self.conv2d = nn.Conv2d(in_channels, out_channels, kernel_size, stride)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_in = x\n",
    "        if self.upsample:\n",
    "            x_in = nn.functional.interpolate(x_in, mode='nearest', scale_factor=self.upsample)\n",
    "        out = self.reflection_pad(x_in)\n",
    "        out = self.conv2d(out)\n",
    "        return out\n",
    "    \n",
    "\n",
    "    \n",
    "class TransformerNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TransformerNet, self).__init__()\n",
    "        # Initial convolution layers\n",
    "        self.encoder = nn.Sequential()\n",
    "        \n",
    "        self.encoder.add_module('conv1', ConvLayer(3, 32, kernel_size=9, stride=1))\n",
    "        self.encoder.add_module('in1', nn.InstanceNorm2d(32, affine=True))\n",
    "        self.encoder.add_module('relu1', nn.ReLU())\n",
    "        self.encoder.add_module('maxpool1', nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.encoder.add_module('conv2', ConvLayer(32, 64, kernel_size=3, stride=2))\n",
    "        self.encoder.add_module('in2', nn.InstanceNorm2d(64, affine=True))\n",
    "        self.encoder.add_module('relu2', nn.ReLU())\n",
    "        self.encoder.add_module('maxpool2', nn.MaxPool2d(2,2))\n",
    "        \n",
    "        self.encoder.add_module('conv3', ConvLayer(64, 128, kernel_size=3, stride=2))\n",
    "        self.encoder.add_module('in3', nn.InstanceNorm2d(128, affine=True))\n",
    "        self.encoder.add_module('relu3', nn.ReLU())\n",
    "        self.encoder.add_module('maxpool3', nn.MaxPool2d(2,2))\n",
    "\n",
    "        # Residual layers\n",
    "        self.residual = nn.Sequential()\n",
    "        \n",
    "        for i in range(5):\n",
    "            self.residual.add_module('resblock_%d' %(i+1), ResidualBlock(128))\n",
    "        \n",
    "        # Upsampling Layers\n",
    "        self.decoder = nn.Sequential()\n",
    "        self.decoder.add_module('upsample1', nn.Upsample(scale_factor=2))\n",
    "        self.decoder.add_module('deconv1', UpsampleConvLayer(128, 64, kernel_size=3, stride=1, upsample=2))\n",
    "        self.decoder.add_module('in4', nn.InstanceNorm2d(64, affine=True))\n",
    "        self.decoder.add_module('relu4', nn.ReLU())\n",
    "\n",
    "        self.decoder.add_module('upsample2', nn.Upsample(scale_factor=2))\n",
    "        self.decoder.add_module('deconv2', UpsampleConvLayer(64, 32, kernel_size=3, stride=1, upsample=2))\n",
    "        self.decoder.add_module('in5', nn.InstanceNorm2d(32, affine=True))\n",
    "        self.decoder.add_module('relu5', nn.ReLU())\n",
    "\n",
    "        self.decoder.add_module('upsample3', nn.Upsample(scale_factor=2))\n",
    "        self.decoder.add_module('deconv3', ConvLayer(32, 3, kernel_size=9, stride=1))\n",
    "\n",
    "        #Classifier\n",
    "\n",
    "    def forward(self, X):\n",
    "        encoder_output = self.encoder(X)\n",
    "        residual_output = self.residual(encoder_output)\n",
    "        decoder_output = self.decoder(residual_output)\n",
    "        return decoder_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Mvql0FDrwCJT"
   },
   "outputs": [],
   "source": [
    "def normalize_batch(batch):\n",
    "    # normalize using imagenet mean and std\n",
    "    mean = batch.new_tensor([0.485, 0.456, 0.406]).view(-1, 1, 1)\n",
    "    std = batch.new_tensor([0.229, 0.224, 0.225]).view(-1, 1, 1)\n",
    "    batch = batch.div_(255.0)\n",
    "    return (batch - mean) / std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfurznYswCJW"
   },
   "outputs": [],
   "source": [
    "np.random.seed(random_seed)\n",
    "torch.manual_seed(random_seed)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(256),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Lambda(lambda x: x.mul(255))\n",
    "])\n",
    "\n",
    "style_transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Lambda(lambda x: x.mul(255))\n",
    "])    \n",
    "\n",
    "train_dataset = datasets.ImageFolder(\"/content/gdrive/My Drive/Colab_Notebooks/best-artworks-of-all-time/images\", transform) #FIXME\n",
    "test_dataset = datasets.ImageFolder(\"/content/gdrive/My Drive/Colab_Notebooks/best-artworks-of-all-time/images\", transform) #FIXME\n",
    "\n",
    "num_train = len(train_dataset)\n",
    "indices = list(range(num_train))\n",
    "split = int(np.floor(0.1* num_train))\n",
    "\n",
    "np.random.seed(random_seed)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_idx, test_idx = indices[split:], indices[:split]\n",
    "train_sampler = SubsetRandomSampler(train_idx)\n",
    "test_sampler = SubsetRandomSampler(test_idx)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler = train_sampler)\n",
    "test_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, sampler = test_sampler)\n",
    "print(\"train dataset num:\", len(train_loader))\n",
    "print(\"test dataset num:\",len(test_loader) )\n",
    "\n",
    "def im_convert(tensor):\n",
    "    image = tensor.clone().detach().numpy()\n",
    "    image = image.transpose(1, 2, 0)\n",
    "    # denormalize\n",
    "    image = image * np.array([0.5, 0.5, 0.5] + np.array([0.5, 0.5, 0.5]))\n",
    "    image = image.clip(0, 255)\n",
    "    return image\n",
    "\n",
    "dataiter = iter(train_loader)\n",
    "images, classes = dataiter.next()\n",
    "title = [train_dataset.classes[i] for i in classes]\n",
    "\n",
    "fig = plt.figure(figsize=(25, 4))\n",
    "\n",
    "for i in np.arange(20):\n",
    "    # row 2 column 10\n",
    "    ax = fig.add_subplot(2, 10, i+1, xticks=[], yticks=[])\n",
    "    plt.imshow(im_convert(images[i]).astype('uint8'))\n",
    "    ax.set_title(title[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Gmz9otAGwCJZ"
   },
   "outputs": [],
   "source": [
    "transformer = TransformerNet().to(device)\n",
    "summary(transformer, (3, 256, 256))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gvr7_Lj5wCJc"
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(transformer.parameters(), initial_lr)\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "transfer_learning = True # inference or training first --> False / Transfer learning --> True\n",
    "ckpt_model_path = os.path.join(checkpoint_dir, \"ckpt_epoch_85_batch_id_200.pth\") #FIXME\n",
    "\n",
    "if transfer_learning:\n",
    "    checkpoint = torch.load(ckpt_model_path, map_location=device)\n",
    "    transformer.load_state_dict(checkpoint['model_state_dict'])\n",
    "    transformer.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "c9AAZUBswCJf"
   },
   "outputs": [],
   "source": [
    "if running_option == 'training':\n",
    "    if transfer_learning:\n",
    "        transfer_learning_epoch = checkpoint['epoch']\n",
    "    else:\n",
    "        transfer_learning_epoch = 0\n",
    "        \n",
    "    for epoch in range(transfer_learning_epoch, num_epochs):\n",
    "        transformer.train()\n",
    "        agg_style_loss=0.\n",
    "        count=0\n",
    "        \n",
    "        for batch_id, (x,labels) in enumerate(train_loader):\n",
    "            n_batch = len(x)\n",
    "            count += n_batch\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # ================= forward =====================\n",
    "            labels = labels.to(device)\n",
    "            x = x.to(device)\n",
    "            y = transformer(x)\n",
    "            y = normalize_batch(y)\n",
    "            x = normalize_batch(x)\n",
    "            \n",
    "            # ================= loss =====================\n",
    "            loss = criterion(y, x)\n",
    "\n",
    "            # =============== Backward ===================\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (batch_id + 1) % log_interval == 0:\n",
    "                mesg = \"{}\\tEpoch {}:\\t[{}/{}]\\ttotal: {:.6f}\".format(\n",
    "                    time.ctime(), epoch + 1, count, len(train_dataset),\n",
    "                                  loss.data\n",
    "                )\n",
    "                print(mesg)\n",
    "            \n",
    "            if checkpoint_dir is not None and (batch_id + 1) % checkpoint_interval == 0:\n",
    "                transformer.eval().cpu()\n",
    "                ckpt_model_filename = \"ckpt_epoch_\" + str(epoch) + \"_batch_id_\" + str(batch_id + 1) + \".pth\"\n",
    "                print(str(epoch), \"th checkpoint is saved!\")\n",
    "                ckpt_model_path = os.path.join(checkpoint_dir, ckpt_model_filename)\n",
    "                torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': transformer.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'loss': loss.data\n",
    "                }, ckpt_model_path)\n",
    "\n",
    "                transformer.to(device).train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q4ZtMuc-UYx1"
   },
   "outputs": [],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "if running_option == \"test\":\n",
    "  with torch.no_grad():\n",
    "        style_model = TransformerNet()\n",
    "        \n",
    "        ckpt_model_path = os.path.join(checkpoint_dir, \"ckpt_epoch_10_batch_id_200.pth\") #FIXME\n",
    "        checkpoint = torch.load(ckpt_model_path, map_location=device)\n",
    "\n",
    "        style_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        style_model.to(device)    \n",
    "        for x, labels in test_loader :\n",
    "            x = x.to(device)\n",
    "            labels = labels.to(device)\n",
    "            y, outputs = style_model(x)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "print('Accuracy of the network on the test images: %d %%' % (\n",
    "    100 * correct / total))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "svtAM1b6b3lo"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "o4cSYtnbb3ir"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FZc9_IXNUUVX"
   },
   "source": [
    "https://excelsior-cjh.tistory.com/187 - sparse loss function\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "PerceptualLoss_Training (CNN+Maxpool)_3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
